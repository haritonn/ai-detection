{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c326610e-95d2-4f40-bfd2-68e48e9570cb",
   "metadata": {},
   "source": [
    "# BERT fine-tuning (LoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138461a6-3b0a-4f5e-b600-e87f3e62046c",
   "metadata": {},
   "source": [
    "## 0. Setting up ClearML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c29269ab-67b9-4d6c-9d3a-caadb268455e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=177849254c27481dba78dad9f2bd0572\n",
      "ClearML results page: https://app.clear.ml/projects/78810acc2d5d484cb6d259425be12de4/experiments/177849254c27481dba78dad9f2bd0572/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not fetch GPU stats: RM has detected an NVML/RM version mismatch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Monitor: GPU monitoring failed getting GPU reading, switching off GPU monitoring\n"
     ]
    }
   ],
   "source": [
    "from clearml import Task\n",
    "\n",
    "task = Task.init(\n",
    "    project_name=\"ai_text_classification\",\n",
    "    task_name=\"bert_with_lora_ft\",\n",
    "    reuse_last_task_id=True,\n",
    "    task_type=Task.TaskTypes.training #by default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e8fbe2-ae46-432b-836e-12446c55662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'lr': 2e-4,\n",
    "    'batch_size': 8,\n",
    "    'num_epochs': 10,\n",
    "    'lora_r': 8,\n",
    "    'lora_alpha': 16,\n",
    "    'lora_dropout': .1,\n",
    "}\n",
    "\n",
    "params = task.connect(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7996a0a-b951-4818-86c9-2fdf006656c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_logger = task.get_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c163be-1312-4908-9d60-7174fa6ff130",
   "metadata": {},
   "source": [
    "## 1. Setting env, loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d458e9e5-443c-4621-b92d-34fb8681332c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haritonn/Code/ai_detection/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:184: UserWarning:\n",
      "\n",
      "CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:119.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"CLEARML_LOG_MODEL\"] = \"True\"\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    ")\n",
    "import evaluate\n",
    "import peft\n",
    "\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52319e6e-435d-462d-9d4f-e6f3878b80c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scenario</th>\n",
       "      <th>Requirement</th>\n",
       "      <th>Requirement Type</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>Travel Planning and Guidance Application</td>\n",
       "      <td>Software should appeal to a wide audience by p...</td>\n",
       "      <td>Nonfunctional</td>\n",
       "      <td>ChatGpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Block chain -based secure voting system</td>\n",
       "      <td>Safety measures that prevent fraud and double ...</td>\n",
       "      <td>Functional</td>\n",
       "      <td>Human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>Pet Maintenance Application</td>\n",
       "      <td>The system should be scaled and can be easily ...</td>\n",
       "      <td>Nonfunctional</td>\n",
       "      <td>ChatGpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Online Education Platform Development and Stud...</td>\n",
       "      <td>Training materials should be presented in diff...</td>\n",
       "      <td>Functional</td>\n",
       "      <td>ChatGpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>Virtual Speech Practical Platform Supporting L...</td>\n",
       "      <td>The system should be scaled and can be easily ...</td>\n",
       "      <td>Nonfunctional</td>\n",
       "      <td>ChatGpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Chatbot Support Customer Service Platform</td>\n",
       "      <td>That data storage and processing processes are...</td>\n",
       "      <td>Nonfunctional</td>\n",
       "      <td>Human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>Pet Maintenance Application</td>\n",
       "      <td>Vaccination tracking of pets and keeping the h...</td>\n",
       "      <td>Functional</td>\n",
       "      <td>ChatGpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>Social Donation and Help Collection Mobile App...</td>\n",
       "      <td>Software should appeal to a wide audience by p...</td>\n",
       "      <td>Nonfunctional</td>\n",
       "      <td>ChatGpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Smart Agriculture Application that gives farme...</td>\n",
       "      <td>Compliance with high safety standards by maint...</td>\n",
       "      <td>Nonfunctional</td>\n",
       "      <td>Human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Energy efficiency improvement application for ...</td>\n",
       "      <td>User account security should be provided with ...</td>\n",
       "      <td>Nonfunctional</td>\n",
       "      <td>ChatGpt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Scenario  \\\n",
       "375           Travel Planning and Guidance Application   \n",
       "47             Block chain -based secure voting system   \n",
       "277                        Pet Maintenance Application   \n",
       "225  Online Education Platform Development and Stud...   \n",
       "357  Virtual Speech Practical Platform Supporting L...   \n",
       "156          Chatbot Support Customer Service Platform   \n",
       "267                        Pet Maintenance Application   \n",
       "315  Social Donation and Help Collection Mobile App...   \n",
       "110  Smart Agriculture Application that gives farme...   \n",
       "216  Energy efficiency improvement application for ...   \n",
       "\n",
       "                                           Requirement Requirement Type  \\\n",
       "375  Software should appeal to a wide audience by p...    Nonfunctional   \n",
       "47   Safety measures that prevent fraud and double ...       Functional   \n",
       "277  The system should be scaled and can be easily ...    Nonfunctional   \n",
       "225  Training materials should be presented in diff...       Functional   \n",
       "357  The system should be scaled and can be easily ...    Nonfunctional   \n",
       "156  That data storage and processing processes are...    Nonfunctional   \n",
       "267  Vaccination tracking of pets and keeping the h...       Functional   \n",
       "315  Software should appeal to a wide audience by p...    Nonfunctional   \n",
       "110  Compliance with high safety standards by maint...    Nonfunctional   \n",
       "216  User account security should be provided with ...    Nonfunctional   \n",
       "\n",
       "      Author  \n",
       "375  ChatGpt  \n",
       "47     Human  \n",
       "277  ChatGpt  \n",
       "225  ChatGpt  \n",
       "357  ChatGpt  \n",
       "156    Human  \n",
       "267  ChatGpt  \n",
       "315  ChatGpt  \n",
       "110    Human  \n",
       "216  ChatGpt  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"../dataset/Dataset.csv\")\n",
    "\n",
    "dataset.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b03bec-dad2-4781-ae72-b6fc7e50f123",
   "metadata": {},
   "source": [
    "## 2. Loading model, splitting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aac269a4-89be-458b-be20-fabcaf608d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "807cebb2-14f7-4cb9-86c0-b5b483e02163",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'google-bert/bert-base-uncased'\n",
    "\n",
    "label2id = {'human': 0, 'chatgpt': 1}\n",
    "id2label = {0: 'human', 1: 'chatgpt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e15afb3-9cb1-4830-927b-97140733e33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062547b2cfb64124b58289d87bdd53b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: google-bert/bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "classifier.bias                            | MISSING    | \n",
      "classifier.weight                          | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tokenizer_bert = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model_bert = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4b8f8b5-f2f4-4ea9-88fd-4caf1e7da5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'User data should be backed regularly and stored safely',\n",
       " 'labels': 1,\n",
       " '__index_level_0__': 218}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(dataset, test_size=.4, stratify=dataset['Author'], random_state=42)\n",
    "test_df, val_df = train_test_split(test_df, test_size=.5, stratify=test_df['Author'], random_state=42)\n",
    "\n",
    "train_df, test_df, val_df = [df.rename(columns={'Requirement': 'text', 'Author': 'labels'}) for df in [train_df, test_df, val_df]]\n",
    "\n",
    "for df in [train_df, test_df, val_df]:\n",
    "    df['labels'] = df['labels'].str.lower().map(label2id)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df[['text', 'labels']]),\n",
    "    \"test\": Dataset.from_pandas(test_df[['text', 'labels']]),\n",
    "    \"validation\": Dataset.from_pandas(val_df[['text', 'labels']]),\n",
    "})\n",
    "\n",
    "dataset_dict['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3b68c-944a-4199-8e06-48042ecfa941",
   "metadata": {},
   "source": [
    "## 3. Tokenization, collating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42a79bbb-438c-4c9a-9c31-f753971c06a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def preprocess_function(example):\n",
    "    example['text'] = example['text'].lower()\n",
    "    tokens = tokenizer_bert(\n",
    "        example['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128\n",
    "    )\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "918dcb19-3d0c-45f7-8703-86844dd0a67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b39627ffe3749f89a6a5f95e7e0ec00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe69614ce6ed49e0b8784205326f629e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2de8554971c412086265f08a777b9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_tokenized = dataset_dict.map(preprocess_function, batched=False)\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f62789-5268-4639-a4d4-3d97af2b611d",
   "metadata": {},
   "source": [
    "## 4. Defining Trainer & metrics computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb9cadb6-b7fa-46b0-8904-ff2891e1429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load('accuracy')\n",
    "roc_auc = evaluate.load('roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a119bfe9-fe5b-42cd-8b1b-9d03aa9e7ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # softmax action\n",
    "    probs = np.exp(predictions) / np.exp(predictions).sum(-1, keepdims=True)\n",
    "    positive_probs = probs[:, 1]\n",
    "    auc = np.round(roc_auc.compute(prediction_scores=positive_probs, references=labels)['roc_auc'], 3)\n",
    "\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    acc = np.round(accuracy.compute(predictions=predicted_classes, references=labels)['accuracy'], 3)\n",
    "\n",
    "    return {\"accuracy\": acc, \"roc_auc\": auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a406cac6-8064-4d26-b1fe-59f6056b0a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 296,450 || all params: 109,780,228 || trainable%: 0.2700\n"
     ]
    }
   ],
   "source": [
    "lora_config = peft.LoraConfig(\n",
    "    task_type=peft.TaskType.SEQ_CLS,\n",
    "    r=params['lora_r'],\n",
    "    lora_alpha=params['lora_alpha'],\n",
    "    lora_dropout=params['lora_dropout'],\n",
    "    target_modules=['query', 'value'],\n",
    "    \n",
    ")\n",
    "\n",
    "model_lora = peft.get_peft_model(model_bert, lora_config)\n",
    "model_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1728c2-80a6-4e35-bbc3-bb828f8f8244",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='../dataset/results',\n",
    "    per_device_train_batch_size=params['batch_size'],\n",
    "    per_device_eval_batch_size=params['batch_size'],\n",
    "    learning_rate=params['lr'],\n",
    "    num_train_epochs=params['num_epochs'],\n",
    "    logging_strategy='epoch',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    model=model_lora,\n",
    "    train_dataset=dataset_tokenized['train'],\n",
    "    eval_dataset=dataset_tokenized['test'],\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9343f68e-2310-4df5-9ce8-c39acbcd8a92",
   "metadata": {},
   "source": [
    "## 5. Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725869e6-6d95-486d-834a-4824efdc719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_history = trainer.state.log_history\n",
    "train_logs = [x for x in log_history if 'loss' in x and 'eval_loss' not in x]\n",
    "eval_logs  = [x for x in log_history if 'eval_loss' in x]\n",
    "\n",
    "train_df = pd.DataFrame(train_logs)[['epoch', 'loss']].rename(columns={'loss': 'train_loss'})\n",
    "eval_df  = pd.DataFrame(eval_logs)[['epoch', 'eval_loss', 'eval_accuracy', 'eval_roc_auc']]\n",
    "\n",
    "results_df = pd.merge(train_df, eval_df, on='epoch')\n",
    "results_df['epoch'] = results_df['epoch'].astype(int)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28e83f9-ff4e-4ee4-b993-c81f798c41aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('BERT fine-tuning results (LoRA)')\n",
    "\n",
    "axes[0].plot(results_df['epoch'], results_df['train_loss'], marker='o', label='Train Loss')\n",
    "axes[0].plot(results_df['epoch'], results_df['eval_loss'],  marker='o', label='Val Loss')\n",
    "axes[0].set_title(\"Loss\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(results_df['epoch'], results_df['eval_accuracy'], marker='o', color='green')\n",
    "axes[1].set_title(\"Validation Accuracy\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].set_ylim(0.9, 1.0)\n",
    "\n",
    "axes[2].plot(results_df['epoch'], results_df['eval_roc_auc'], marker='o', color='orange')\n",
    "axes[2].set_title(\"Validation ROC-AUC\")\n",
    "axes[2].set_xlabel(\"Epoch\")\n",
    "axes[2].set_ylabel(\"ROC-AUC\")\n",
    "axes[2].set_ylim(0.9, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a22899-65f7-4545-b7bb-e6ea72251809",
   "metadata": {},
   "source": [
    "## 6. Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c943d2-a8cc-4dc9-bbf2-955be24434f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
